{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归\n",
    "\n",
    "## 符号描述\n",
    "假设有训练数据集$D= \\left \\{(x_1, y_1), (x_2, y_2), \\cdots, (x_m, y_m) \\right \\}$\n",
    "其中第$i^{th}$个样本$x_i$有n个特征，它的特征向量为n维列向量。\n",
    "$$\n",
    "{x}_i = \\begin{pmatrix}\n",
    "x_{i,1} \\\\\n",
    "x_{i,2} \\\\\n",
    "{\\vdots} \\\\\n",
    "x_{i,n} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "则m个输入样本，输入矩阵为$m \\times n$维的矩阵。\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "(x_1)^T \\\\\n",
    "(x_2)^T \\\\\n",
    "{\\vdots} \\\\\n",
    "(x_m)^T \n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\\n",
    "x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "m个输出向量为：\n",
    "$$\n",
    "y = \\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_m\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "线性回归就是寻求自变量与因变量之间的线性关系，假设函数为：\n",
    "$$\n",
    "h_{\\theta}(x_i) = \\theta_0 + \\theta_1x_{i,1} + \\theta_2x_{i,2} + \\cdots + \\theta_nx_{i,n}\n",
    "$$\n",
    "\n",
    "其中，$\\theta$写成向量形式如：\n",
    "$$\n",
    "\\theta = \\begin{pmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "给向量$x_i$加上元素$x_{i,0} = 1$，则假设函数可以用向量表示为：\n",
    "$$h_{\\theta}(x_i) = \\theta^Tx_i$$\n",
    "全部m个样本输入到$h_{\\theta}$中为：\n",
    "$$h_{\\theta}(X) = \\theta^TX$$\n",
    "\n",
    "其中X为：\n",
    "$$\n",
    "X\n",
    "= \\begin{pmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\\n",
    "1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型\n",
    "\n",
    "假设第$i^{th}$个样本经过假设函数输出为：\n",
    "$$\n",
    "\\hat{y_i} = h_{\\theta}(x_i)\n",
    "$$\n",
    "线性回归试图学到一个$h_{\\theta}$使得$\\hat{y_i} \\approx y_i$。也就是说使得模型的输出尽可能接近真实值。\n",
    "\n",
    "整个训练集输入到假设函数里面有\n",
    "$$\n",
    "\\hat{y} = h_{\\theta}(X) = X\\theta\n",
    "$$\n",
    "\n",
    "为衡量假设输出与真实输出的误差，需要定义一个损失函数$loss(\\hat{y_i}, y_i)$，它度量了假设输出与真实值之间的损失，这个函数的结果越小表示越接近。\n",
    "通常在回归模型中，常用均方误差作为损失的度量。\n",
    "$$\n",
    "loss(\\hat{y_i}, y_i) = \\left ( \\hat{y_i} - y_i\\right)^2 = \\left ( h_{\\theta}(x_i) - y_i\\right)^2 \n",
    "$$\n",
    "\n",
    "整个训练集上的损失常用代价函数描述，它是关于$\\theta$的函数：\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m loss\\left(\\hat{y_i}, y_i \\right) = \\frac{1}{2m} \\sum_{i=1}^m \\left(\\theta^Tx_i - y_i \\right)^2\n",
    "$$\n",
    "\n",
    "线性模型的目标为求得一组$\\theta$使得$J(\\theta)$最小，用数学语言描述为：\n",
    "$$\\theta^* = \\underset{\\theta}{arg min} J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化\n",
    "上面建立了数学模型，接下来就要想办法求得以上模型中参数的最优解。\n",
    "###  梯度下降法\n",
    "为什么认为$J(\\theta)$可以用梯度下降法求得全局最优解？\n",
    "因为该函数为凸函数，凸函数可以收敛到全局最优。\n",
    "\n",
    "为什么认为$J(\\theta)$为凸函数？\n",
    "将其展开。\n",
    "$$\n",
    "J(\\theta_0, \\theta_1, \\cdots, \\theta_n) = \\frac{1}{2m} \\sum_{i=1}^m\\left(\\theta_0x_{i,0} + \\theta_1x_{i,1} + \\theta_nx_{i,n} - y_i\\right)^2\n",
    "$$\n",
    "\n",
    "观察上式，$J(\\theta_i)$在固定其他$\\theta$时它是一个关于$\\theta_i$的二次函数。\n",
    "\n",
    "举个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13ff21ff710>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'theta 0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'J(theta)')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x13ff2537eb8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 以波士顿房价数据集作为例子\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "X,y = datasets.load_boston(return_X_y=True)\n",
    "m, n = X.shape\n",
    "\n",
    "theta = np.random.rand(n,1)\n",
    "\n",
    "def compute_cost(theta, X, y):\n",
    "    '''\n",
    "    theta: n * 1\n",
    "    X: m * n\n",
    "    y: m * 1\n",
    "    '''\n",
    "    r = np.dot(X, theta) - y\n",
    "    result = np.dot(r.T, r) / (2*m)\n",
    "    return result[0][0]\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "# x = np.linspace(0,10,100)\n",
    "x_range = np.linspace(-100,100,300)\n",
    "y_range = []\n",
    "for x in x_range:\n",
    "    theta[0] = x\n",
    "    y_range.append(compute_cost(theta, X, y))\n",
    "\n",
    "y_range = np.array(y_range)\n",
    "plt.plot(x_range, y_range, label=\"J(theta)\")\n",
    "plt.xlabel(\"theta 0\")\n",
    "plt.ylabel(\"J(theta)\")\n",
    "plt.legend() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的图中为固定其他$\\theta$,只变化$\\theta_0$时的$J(\\theta_0)$结果，明显可以看出它是典型的二次函数，有全局最小值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求梯度\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\frac{1}{m} \\sum_{i=1}^m (\\theta^T x_i -y_i) x_{i,k}\n",
    "$$\n",
    "\n",
    "写成向量形式为：\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta} = X^T(X\\theta -y)\n",
    "$$\n",
    "\n",
    "因此，每次迭代更新$\\theta$的公式为：\n",
    "$$\n",
    "\\theta = \\theta - \\alpha( \\frac{\\partial J(\\theta)}{\\partial \\theta} = \\theta - \\alpha  X^T(X\\theta -y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.124517786285605"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9.33236812e-01],\n",
       "       [-9.43835567e-02],\n",
       "       [ 5.17531695e-02],\n",
       "       [-1.09617983e-02],\n",
       "       [ 1.39115404e+00],\n",
       "       [ 2.64408146e-01],\n",
       "       [ 5.57053562e+00],\n",
       "       [-5.28666389e-03],\n",
       "       [-9.31576163e-01],\n",
       "       [ 1.83825528e-01],\n",
       "       [-1.06359096e-02],\n",
       "       [-3.80455161e-01],\n",
       "       [ 1.51231562e-02],\n",
       "       [-4.52803009e-01]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用代码实现梯度下降的优化\n",
    "class LR:\n",
    "    def __init__(self, X, y):\n",
    "        self.m = X.shape[0] # 样本数量\n",
    "        self.n = X.shape[1] + 1 # 特征数目\n",
    "        self.X = np.insert(X, 0, values=np.ones(self.m), axis=1)\n",
    "        self.y = y.reshape((-1, 1))\n",
    "        self.theta = np.random.rand(self.n,1)\n",
    "        self.learning_rate = 0.00000001\n",
    "        self.num_iter = 1000000\n",
    "    \n",
    "    def compute_cost(self, theta):\n",
    "        '''\n",
    "        theta: n * 1\n",
    "        X: m * n\n",
    "        y: m * 1\n",
    "        '''\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        r = np.dot(X, theta) - y\n",
    "        result = np.dot(r.T, r) / (2*m)\n",
    "        return result[0][0]\n",
    "    \n",
    "    def compute_grad(self, theta, X, y):\n",
    "        '''\n",
    "        theta: n * 1\n",
    "        X: m * n\n",
    "        y: m * 1\n",
    "        '''\n",
    "        res = np.dot(X, theta) - y\n",
    "        result = np.dot(X.T, res)\n",
    "        return result\n",
    "\n",
    "    def optimizer(self):\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        theta = self.theta\n",
    "        learning_rate = self.learning_rate\n",
    "        num_iter = self.num_iter\n",
    "        \n",
    "        \n",
    "        for i in range(num_iter):\n",
    "            grad = compute_grad(theta, X, y)\n",
    "            theta = theta - learning_rate * grad\n",
    "        return theta\n",
    "\n",
    "lr = LR(X, y)\n",
    "\n",
    "theta = lr.optimizer()\n",
    "lr.compute_cost(theta)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵求导法求最优解\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left(\\theta^Tx_i - y_i \\right)^2\n",
    "$$\n",
    "\n",
    "写成矩阵形式\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}(X\\theta -y)^T(X\\theta -y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标函数为连续函数，目标函数取得最小值时$\\theta$一定为目标函数驻点，它的梯度在最优值处一定为0。\n",
    "将$J(\\theta)$展开，前面的系数为常数，对结果无任何影响可以省略掉。\n",
    "$$\n",
    "J(\\theta) = (X\\theta -y)^T(X\\theta - y) = \\theta^TX^TX\\theta - y^TX\\theta - \\theta^TX^Ty + y^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式中，其中$ y^TX\\theta$ 和 $\\theta^TX^Ty $都是标量，他们的值应该是相等的，为了后面求导方便，统一写成$\\theta^TX^Ty$，于是.\n",
    "$$\n",
    "J(\\theta) = \\theta^TX^TX\\theta - 2\\theta^TX^Ty + y^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运用矩阵求导公式得到：\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta} = X^TX\\theta + (X^TX)^T\\theta - 2X^Ty = 2X^TX\\theta - 2X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令梯度等于0\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta} = 0 \\Rightarrow X^TX\\theta - X^Ty = 0 \\\\ \n",
    "\\Rightarrow  X^TX\\theta = X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X^TX$为$m \\times m$的方阵，如果$X^TX$是可逆的，可以通过两边同时左乘以$(X^TX)^{-1}$来求得$\\theta$，即。\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.947415590864596"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.64594884e+01],\n",
       "       [-1.08011358e-01],\n",
       "       [ 4.64204584e-02],\n",
       "       [ 2.05586264e-02],\n",
       "       [ 2.68673382e+00],\n",
       "       [-1.77666112e+01],\n",
       "       [ 3.80986521e+00],\n",
       "       [ 6.92224640e-04],\n",
       "       [-1.47556685e+00],\n",
       "       [ 3.06049479e-01],\n",
       "       [-1.23345939e-02],\n",
       "       [-9.52747232e-01],\n",
       "       [ 9.31168327e-03],\n",
       "       [-5.24758378e-01]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#代码实现\n",
    "\n",
    "class LR2:\n",
    "    def __init__(self, X, y):\n",
    "        self.m = X.shape[0] # 样本数量\n",
    "        self.n = X.shape[1] + 1 # 特征数目\n",
    "        self.X = np.insert(X, 0, values=np.ones(self.m), axis=1)\n",
    "        self.y = y.reshape((-1, 1))\n",
    "        \n",
    "    def normal_equation(self):\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "        return theta\n",
    "    \n",
    "    def compute_cost(self, theta):\n",
    "        '''\n",
    "        theta: n * 1\n",
    "        X: m * n\n",
    "        y: m * 1\n",
    "        '''\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        \n",
    "        r = np.dot(X, theta) - y\n",
    "        result = np.dot(r.T, r) / (2*m)\n",
    "        return result[0][0]\n",
    "\n",
    "    \n",
    "lr = LR2(X, y)\n",
    "theta = lr.normal_equation()\n",
    "lr.compute_cost(theta)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.08011358e-01,  4.64204584e-02,  2.05586264e-02,\n",
       "         2.68673382e+00, -1.77666112e+01,  3.80986521e+00,\n",
       "         6.92224640e-04, -1.47556685e+00,  3.06049479e-01,\n",
       "        -1.23345939e-02, -9.52747232e-01,  9.31168327e-03,\n",
       "        -5.24758378e-01]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([36.45948839])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 与sklearn的结果比较\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "lr.coef_\n",
    "lr.intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面代码运行的结果来看，用矩阵求导法求出来的最优解与sklearn求出来的一样，结果比用梯度下降要好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概率上的意义\n",
    "为什么在线性回归中选择均方误差作为损失函数是可行的，因为它有着概率上的意义。\n",
    "### 中心极限定理\n",
    "\n",
    "**当样本量N逐渐趋于无穷大时，N个抽样样本的均值的频数逐渐趋于正态分布**，其对原总体的分布不做任何要求，意味着无论总体是什么分布，其抽样样本的均值的频数的分布都随着抽样数的增多而趋于正态分布。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用极大似然估计解释最小二乘法\n",
    "对于给定输入样本$x$, 我们得到预测值$\\hat{y}$与真实值$y$之间存在误差$\\epsilon$，即。\n",
    "$$\n",
    "y = \\hat{y} + \\epsilon\n",
    "$$\n",
    "对m个输入样本有m个$\\epsilon$，它们是独立同分布的。\n",
    "根据中心极限定理，由于误差项是好多好多相互独立的因素影响的综合影响，我们有理由假设其服从高斯分布，并且它们的均值为0，方差为某定值$\\sigma$.\n",
    "$$\n",
    "\\epsilon \\sim N(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "因此有\n",
    "$$\n",
    "(y-\\widehat{y}) \\sim N(0, \\sigma^2) \\\\\n",
    "=> y \\sim N(\\widehat{y}, \\sigma^2) \n",
    "$$\n",
    "\n",
    "其中\n",
    "$$\n",
    "\\hat{y} = h_{\\theta}(x) = \\theta^Tx\n",
    "$$\n",
    "\n",
    "即\n",
    "$$\n",
    "P(y|x; \\theta) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{(y-\\theta^Tx)^2}{2\\sigma^2})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知m个$y$服从正太分布，并且这m个$y$真实出现了，那么什么样的参数会使得这m个$y$出现的概率最大呢？这可以用极大似然估计来求解。似然函数为：\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^mP(y_i|x_i;\\theta) = \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{(y_i-\\theta^Tx_i)^2}{2\\sigma^2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两边取对数得：\n",
    "$$\n",
    "l(\\theta) = logL(\\theta) = \\sum_{i=1}^mlog\\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp(-\\frac{(y_i-\\theta^Tx_i)^2}{2\\sigma^2}) \\right) \\\\\n",
    "= \\sum_{i=1}^m \\left( log\\frac{1}{\\sqrt{2\\pi \\sigma^2}} + log exp(-\\frac{(y_i-\\theta^Tx_i)^2}{2\\sigma^2})  \\right) \\\\\n",
    "= m log\\frac{1}{\\sqrt{2\\pi \\sigma^2}} - \\sum_{i=1}^m \\frac{(y_i-\\theta^Tx_i)^2}{2\\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察$l(\\theta0$，式子中$\\sigma$为定值，因此，要想最大化$l(\\theta)$就是要使$\\sum_{i=1}^m(y_i - \\theta^Tx_i)^2$最小，这与我们最小二乘法中损失函数采用均方误差一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 扩展\n",
    "### 正则化\n",
    "#### L2-norm Ridge回归\n",
    "#### L1-norm Lasso回归\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
