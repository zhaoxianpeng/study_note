---
typora-copy-images-to: ./
---

# 神经网络

[TOC]

------



## 前言

通用神经网络架构

![神经网络](./神经网络.png)

### 符号描述

| 符号                | 维度                                                     | 描述                                                         |
| ------------------- | -------------------------------------------------------- | ------------------------------------------------------------ |
| $n$                 | $\mathbb{R}$                                             | 训练样本个数                                                 |
| $m$                 | $\mathbb{R}$                                             | 每个样本的特征个数                                           |
| $u$                 | $\mathbb{R}$                                             | 输出值个数                                                   |
| $x_{i,j}$           | $\mathbb{R}$                                             | 表示第$i^{th}$个输入样本的第$j^{th}$维特征的输入数据         |
| $x_{i}$             | $\mathbb{R}^{m \times 1}$                                | 表示第$i^{th}$个输入样本的所有m维特征向量                    |
| $X$                 | $\mathbb{R}^{m\times n}$                                 | 表示所有$n$个样本，每个样本有$m$个特征                       |
| $w_{s,t}^{(l)}$     | $\mathbb{R}$                                             | 表示从第$(l-1)^{th}$层的第$s^{th}$个神经元到第$l^{th}$层的$t^{th}$个神经元的权重； |
| $\left | l\right |$ | $\mathbb{R}$                                             | 表示第$l^{th}$层神经元个数；                                 |
| $W^{(l)}$           | $\mathbb{R}^{\left | l-1 \right |\times \left|l\right|}$ | 表示从第$(l-1)^{th}$层到$l^{th}$层映射的权重矩阵;            |
| $f$                 | $\mathbb{R} \rightarrow \mathbb{R}$                      | 激活函数                                                     |
| $z_{i,j}^{(l)}$     | $\mathbb{R}$                                             | 表示输入为第$i^{th}$个样本时，第$l^{th}$层的第$j^{th}$个神经元未激活的值，它是由第$(l-1)^{th}$层的输出经过$W^{(l)}$矩阵加权求和之后的结果。 $z_{i,j}^{(l)} = \sum_{k}^{\left|l-1\right|}a_{i,k}^{(l-1)} \cdot w_{k,j}^{(l)}$ |
| $z_{i}^{(l)}$       | $\mathbb{R}^{\left|l\right| \times 1}$                   | 表示输入为第$i^{th}$个样本时，第$l^{th}$层的$\left|l\right|$个神经元的未激活值组成的向量 |
| $Z^{(l)}$           | $\mathbb{R}^{\left|l\right|\times n}$                    | 表示$n$个样本全部输入时，第$l^{th}$层的n个样本，每个样本$\left|l\right|$个神经元未激活值组成的矩阵 |
| $a_{i,j}^{(l)}$     | $\mathbb{R}$                                             | 表示输入为第$i^{th}$个样本时，第$l^{th}$层的第$j^{th}$个神经元的输出，它是由$z_{i,j}^{(l)}$经过激活函数$f$映射而来 |
| $a_{i}^{(l)}$       | $\mathbb{R}^{\left|l\right| \times 1}$                   | 表示输入为第$i^{th}$个样本时，第$l^{th}$层的$\left|l\right|$个神经元的输出向量 |
| $A^{(l)}$           | $\mathbb{R}^{\left|l\right|\times n}$                    | 表示$n$个样本全部输入时，第$l^{th}$层的n个样本，每个样本$\left|l\right|$个神经元的输出组成的矩阵 |
| $\hat{y}_{i,j}$     | $\mathbb{R}$                                             | 表示输入为第$i^{th}$个样本时，输出层的第$j^{th}$个神经元的输出，也是整个模型的第$j^{th}$个的输出 |
| $\hat{y}_{i}$       | $\mathbb{R}^{u \times 1}$                                | 表述输入为第$i^{th}$个样本时，输出层的$u$个神经元的输出向量  |
| $\hat{Y}$           | $\mathbb{R}^{u \times n}$                                | 表示$n$个样本全部输入时，输出层的n个样本输出，每个样本$u$个输出组成的矩阵 |
| $y_{i,j}$           | $\mathbb{R}$                                             | 表示第$i^{th}$个输入样本的第$j^{th}$个实际输出值             |
| $y_i$               | $\mathbb{R}^{u \times 1}$                                | 表述第$i^{th}$个样本输出的$u$个实际输出向量                  |
| $Y$                 | $\mathbb{R}^{u \times n}$                                | 表示所有n个样本的的实际输出，每个样本$u$个实际输出组成的矩阵 |

### 常用公式



## 基本思想

正向传播时,输入样本从输入层传入,经各隐层逐层处理后,传向输出层。若输出层的实际输出与期望的输出(教师信号)不符,则转入误差的反向传播阶段。

反向传播时，将误差以某种形式通过隐层向输入层逐层反传,并将误差分摊给各层的所有单元,从而获得各层单元的误差信号,此误差信号即作为修正各单元权值的依据。

Q: 反向传播的是什么？

A: 误差，利用这些误差调整层之间的权重。

Q: 目的是什么？

A: 找到一组权重，使得整个神经网络模型的输出尽可能的接近实际值，也就是每次迭代误差越来越小。

Q: 怎么表示误差？

A: 有很多误差函数可以表示误差，取决你要用神经网络来做什么，如果是回归模型，常用的均方误差，如果是分类问题，常用的有对数损失，合页损失等。

## 正向传播

单样本$i^{th}$正向传播从第$(l-1)^{th}$层到第$l^{th}$层的公式：
$$
z_{i,j}^{(l)} = \sum_{k}^{\left|l-1\right|}a_{i,k}^{(l-1)} \cdot w_{k,j}^{(l)}
$$
经过激活函数之后：
$$
a_{i,j}^{(l)} = f(z_{i,j}^{(l)})
$$
样本$i^{th}$正向传播从第$(l-1)^{th}$层到第$l^{th}$层写成向量形式：
$$
z_i^{(l)} = \left(W^{(l)}\right)^Ta_{i}^{(l-1)}
$$
经过激活函数
$$
a_{i}^{(l)} = f(z_{i}^{(l)})
$$
输入为多样本时，正向传播从第$(l-1)^{th}$层到第$l^{th}$层的公式：
$$
Z^{(l)}= \left(W^{(l)}\right)^TA^{(l-1)}
$$


假设$A^{(0)}$为输入层，即为输入矩阵$X$维度为：$m\times n$；所有的$W$初始化为随机值；

其中，$W^{(1)}$的维度为$m \times r$，其中$r$为第二层的神经元个数。应用公式(5)可以求出$Z^{(1)}$,维度为$r\times n$。然后对每个元素应用激活函数可以求出$A^{(1)}$，在继续应用公式(5)可以求出$Z^{(2)}$，依次递推可以求出神经网络中所有神经元的值。

## 反向传播

###　损失函数和代价函数

本文针对单个样本的预测值与真实值之间的误差使用损失函数描述；对于全部n个样本的误差采用代价函数来描述。

常用的误差函数有均方误差，交叉熵损失，合页损失，对数损失等，不管具体用什么损失函数，我们定义我们的损失函数为$loss(\hat{y}_{i,j},y_{i,j})$，这里该函数可以是任何合理的损失函数，他的两个参数都为标量，它度量了预测值与实际值的差距。

第$i^{th}$个样本的损失函数：
$$
L(\hat{y}_i, y_i) =  \sum_{j=1}^u loss(\hat{y}_{i,j}, y_{i,j})
$$

代价函数为：
$$
C(\hat{Y}, Y) = \frac{1}{n} \sum_{i=1}^{n}L(\hat{y}_i, y_i) = \sum_{i=1}^n\left[ \sum_{j=1}^u loss(\hat{y}_{i,j}, y_{i,j}) \right]
$$



### 反向传播过程

![ANN example](./ANN example.png)



以上面的一个神经网络模型为例，剖析正向传播和反向传播的过程。

#### 单样本情况

先看单个样本时反向传播的过程，假设选择第$i^{th}$个样本作为研究对象。用$L_{i,j}^{(l)}$表示第$l^{th}$层神经元的第$j^{th}$个神经元的损失（误差），这里$i$表示第$i^{th}$个样本；并假设输出层为第$L^{th}$层。

##### 第一步 输出层传播到输出层的权重矩阵

为了使$L_{i,j}^{(L)}$减小，需要将$w_{*,j}^{(L)}$往负梯度方向移动，即。
$$
w_{k,j}^{(L)} = w_{k,j}^{(L)} - \eta \frac{\partial L_{i,j}^{(L)}}{\part w_{k,j}^{(L)}}
$$
写成向量形式为：
$$
W^{(L)} = W^{(L)} - \eta \frac{\part L_i^{(L)}}{\part W^{(L)}}
$$
其中$L_i^{(l)}$为标量，它为第$l^{th}$所有损失之和，即。
$$
L_i^{(l)} = \sum_{j=1}^{\left| l \right|}L_{i,j}^{(l)}
$$
将$W^{(L)}$看为自变量，$L_i^{(L)}$看做因变量，则式$\frac{\part L_i^{(L)}}{\part W^{(L)}}$为该映射的梯度矩阵，维度和$W^{(L)}$的维度一样。

##### 第二步 输出层传播的倒数第二层的神经元



##### 第三步 倒数第二层的神经元到倒数第二层的权重矩阵



上面公式中，都为已知量，在正向传播中算出来的，这一步可以很轻松算出。

比较麻烦的是计算$W^{(L-1)}$里面各个元素的移动方向，原理还是一样的
$$
w_{k,v}^{(L-1)} = w_{k,v}^{(L-1)} - \eta \frac{\partial L_{i,j}^{(L)}}{\part w_{k,v}^{(L-1)} }
$$
看起来和公式8一样，没什么区别，但是注意公式9的偏导项，$w_{k,j}^{(L-1)}$不是直接影响到$L_{ij}^{(L)}$的，中间还隔着隐藏层$z_i^{(L-1)}$和$a_i^{(L-1)}$的神经元，因此无法直接求得，需要用链式求导法则。
$$
\begin{align}
\frac{\partial L_{i,j}^{(l)}}{\part w_{k,v}^{(l-1)}} 
& = 
    \frac{\part L_{i,j}^{(l)}}{\part a_{i,v}^{(l-1)}} \cdot 
    \frac{\part a_{i,v}^{(l-1)}}{\part w_{k,v}^{(l-1)}} \\
& = \frac{\part L_{i,j}^{(l)}}{\part a_{i,v}^{(l-1)}} \cdot 
    \frac{\part a_{i,v}^{(l-1)}}{\part z_{i,v}^{(l-1)}} \cdot
    \frac{\part z_{i,v}^{(l-1)} }{\part w_{k,v}^{(l-1)}}
\end{align}
$$


写成矩阵形式，同样，将$W^{(l-1)}$看做自变量，$L_i^{(l)}$看做因变量，求梯度矩阵$\frac{\part L_i^{(l)}}{\part W^{(l-1)}}$。

在求梯度矩阵之前，我们先将正向传播从$W^{(l-1)}$到损失$L_i^{(l)}$的过程列出。
$$
\begin{align}
L_i^{(l)} 
&= \sum_{j=1}^{\left| l \right|}L_{i,j}^{(l)} \\
&= \sum_{j=1}^{\left| l \right|}loss(\hat{y}_{i,j}, y_{i,j}) \\
&= \sum_{j=1}^{\left| l \right|}loss\left(\hat{y}_{i,j}, y_{i,j}\right)
\end{align}
$$
其中$\hat{y}_{i,j}$是由自变量$W^{(l-1)}$计算而来，我看先看下正向计算过程。
$$
\hat{y}_{i,j} = f()
$$
